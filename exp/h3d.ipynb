{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/data/projects/punim2016/zexianh/LMSeg\")\n",
    "\n",
    "import pyvista as pv\n",
    "pv.start_xvfb()  \n",
    "\n",
    "w_size = (1000, 1000)\n",
    "zoom = 1.3\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from data.dataset import H3DDataset\n",
    "\n",
    "h3d_set = H3DDataset(root='data/H3DBenchmark/', split='val', epoch='Epoch_March2018')\n",
    "idx = 1\n",
    "\n",
    "mesh = pv.read(h3d_set.processed_mesh_list[idx])\n",
    "\n",
    "face_v = h3d_set[idx].pos.cpu().numpy()\n",
    "face_adj = h3d_set[idx].edge_index.cpu().t().numpy()\n",
    "face_adj = np.hstack((np.full((face_adj.shape[0], 1), 2), face_adj))\n",
    "\n",
    "f_rgba = h3d_set[idx].face_rgba.cpu().numpy() \n",
    "f_normal = h3d_set[idx].normals[:, 0:3].cpu().numpy()\n",
    "\n",
    "mask = h3d_set[idx].y.cpu().numpy()\n",
    "mask_rgba = np.array([h3d_set.rgba_dict[int(label)] for label in mask.tolist()], dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh.cell_data['RGBA'] = f_rgba\n",
    "mesh.cell_data.active_scalars_name = 'RGBA'\n",
    "mesh.plot(scalars='RGBA', \n",
    "          rgb=True, \n",
    "          cpos='iso', \n",
    "          window_size=w_size, \n",
    "          zoom=zoom,\n",
    "          show_axes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh.cell_data['Mask'] = mask_rgba\n",
    "mesh.cell_data.active_scalars_name = 'Mask'\n",
    "mesh.plot(scalars='Mask', \n",
    "          rgb=True,\n",
    "          cpos='iso', \n",
    "          window_size=w_size,\n",
    "          zoom=zoom,\n",
    "          show_edges=False,\n",
    "          show_axes=False,\n",
    "          show_scalar_bar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_pcd = pv.PolyData(face_v)\n",
    "face_pcd.lines = face_adj\n",
    "\n",
    "face_pcd.point_data['RGBA'] = f_rgba\n",
    "face_pcd.point_data.active_scalars_name = 'RGBA'\n",
    "face_pcd.plot(scalars='RGBA', \n",
    "              rgb=True, \n",
    "              cpos='iso', \n",
    "              window_size=w_size, \n",
    "              zoom=zoom,\n",
    "              render_points_as_spheres=True, \n",
    "              point_size=3, \n",
    "              line_width=1,\n",
    "              show_axes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_pcd.point_data['Mask'] = mask_rgba\n",
    "face_pcd.point_data.active_scalars_name = 'Mask'\n",
    "face_pcd.plot(scalars='Mask', \n",
    "              rgb=True,\n",
    "              cpos='iso', \n",
    "              window_size=w_size, \n",
    "              zoom=zoom,\n",
    "              render_points_as_spheres=True, \n",
    "              point_size=3, \n",
    "              line_width=1,\n",
    "              show_axes=False,\n",
    "              show_scalar_bar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surface Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_pcd.point_data['Normal'] = f_normal\n",
    "arrows = face_pcd.glyph(orient='Normal', scale=False, factor=0.05, tolerance=0.005)\n",
    "arrows.plot(cpos='iso', \n",
    "            window_size=w_size,\n",
    "            zoom=zoom,\n",
    "            show_axes=False,\n",
    "            show_scalar_bar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Batch\n",
    "from torchmetrics.classification import Accuracy, F1Score, ConfusionMatrix\n",
    "\n",
    "from train.trainer import Trainer\n",
    "from model.net import LMSegNet, HGAPNet, LGAPNet, GANet\n",
    "\n",
    "\n",
    "with open('cfg/h3d/lmseg_feature.yaml', 'r') as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "    cfg['device'] = 'cpu'\n",
    "    cfg['path'] = 'save/h3d/lmseg_feature'\n",
    "    cfg['epoch'] = '180'\n",
    "    model = LMSegNet(cfg['in_channels'], \n",
    "                     cfg['out_channels'],\n",
    "                     cfg['hid_channels'], \n",
    "                     cfg['num_convs'], \n",
    "                     cfg['pool_factors'], \n",
    "                     cfg['num_nbrs'],\n",
    "                     cfg['num_block'],\n",
    "                     cfg['alpha'], \n",
    "                     cfg['beta'])\n",
    "\n",
    "trainer = Trainer(cfg=cfg) \n",
    "model = trainer.load_weights(model, f\"epoch{cfg['epoch']}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h3d_set = H3DDataset(root='data/H3DBenchmark/', split='val', epoch='Epoch_March2018')\n",
    "test_loader = DataLoader(h3d_set, \n",
    "                         batch_size=cfg['batch'], \n",
    "                         shuffle=False, \n",
    "                         num_workers=cfg['workers'])\n",
    "metric_dict = {\n",
    "    'OA': Accuracy(\n",
    "        task=\"multiclass\", \n",
    "        num_classes=cfg['out_channels'], \n",
    "        ignore_index=11,\n",
    "        average='micro'\n",
    "    ),\n",
    "    'mF1': F1Score(\n",
    "        task=\"multiclass\", \n",
    "        num_classes=cfg['out_channels'], \n",
    "        ignore_index=11,\n",
    "        average='macro'\n",
    "    ),\n",
    "    'F1': F1Score(\n",
    "        task=\"multiclass\", \n",
    "        num_classes=cfg['out_channels'], \n",
    "        ignore_index=11,\n",
    "        average=None\n",
    "    ),\n",
    "}\n",
    "\n",
    "cm = trainer.eval(model, \n",
    "                  test_loader, \n",
    "                  metric=metric_dict, \n",
    "                  ckpt=f\"epoch{cfg['epoch']}.pth\",\n",
    "                  verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y = model(Batch.from_data_list([h3d_set[idx]]).to(cfg['device']))['y'].detach().cpu().numpy()\n",
    "    y = np.argmax(y, axis=-1, keepdims=False)\n",
    "\n",
    "pred = np.array([h3d_set.rgba_dict[int(pred)] for pred in y.tolist()], dtype=np.uint8) \n",
    "\n",
    "mesh = pv.read(h3d_set.processed_mesh_list[idx])\n",
    "mask = h3d_set[idx].y.cpu().numpy()\n",
    "\n",
    "mesh.cell_data['Pred'] = pred\n",
    "mesh.cell_data.active_scalars_name = 'Pred'\n",
    "mesh.plot(scalars='Pred', \n",
    "          rgb=True, \n",
    "          cpos='iso', \n",
    "          window_size=w_size,           \n",
    "          zoom=zoom,\n",
    "          show_axes=False,\n",
    "          show_scalar_bar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = (y != mask)\n",
    "error[mask == 11] = 0\n",
    "\n",
    "mesh.cell_data['Error'] = error\n",
    "mesh.cell_data.active_scalars_name = 'Error'\n",
    "mesh.plot(scalars='Error', \n",
    "          cpos='iso', \n",
    "          cmap=['white', 'red'],\n",
    "          window_size=w_size,           \n",
    "          zoom=zoom,\n",
    "          show_axes=False,\n",
    "          show_scalar_bar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = {}\n",
    "\n",
    "def hook_encoder(name):\n",
    "    def _hook(mod, inp, out):\n",
    "        pos_down, x_down, batch_down, edge_index_down = out\n",
    "        activations[f'{name}/pos_down']   = [p.detach().cpu().numpy() for p in pos_down]\n",
    "        activations[f'{name}/batch_down'] = [b.detach().cpu().numpy() for b in batch_down]\n",
    "        activations[f'{name}/edge_down']  = [e.detach().cpu().numpy() for e in edge_index_down]\n",
    "    return _hook\n",
    "\n",
    "def hook_tensor(name):\n",
    "    def _hook(mod, inp, out):\n",
    "        activations[name] = out.detach().cpu().numpy()  # [N_pool, d]\n",
    "    return _hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(Batch.from_data_list([h3d_set[idx]]).to(cfg['device']))\n",
    "    Z = out['y'].detach().cpu()  # [N, C] logits\n",
    "\n",
    "P = F.softmax(Z, dim=-1).numpy()\n",
    "entropy = -(P * (np.log(P + 1e-12))).sum(axis=1)\n",
    "# Normalize for display\n",
    "emin, emax = entropy.min(), entropy.max()\n",
    "entropy_norm = (entropy - emin) / (emax - emin + 1e-12)\n",
    "\n",
    "mesh.cell_data['Entropy'] = entropy_norm\n",
    "mesh.cell_data.active_scalars_name = 'Entropy'\n",
    "\n",
    "mesh.plot(\n",
    "    scalars='Entropy',\n",
    "    cpos='iso',\n",
    "    window_size=(1000, 1150), \n",
    "    zoom=zoom,\n",
    "    cmap='viridis',            # perceptual; high=uncertain\n",
    "    show_axes=False,\n",
    "    show_scalar_bar=True,\n",
    "    scalar_bar_args={\n",
    "        \"title\": \"Uncertainty (Entropy)\",\n",
    "        \"position_x\": 0.2,   # horizontal placement (0 = left, 1 = right)\n",
    "        \"position_y\": 0.0,   # vertical placement (lower values push bar down),\n",
    "        \"title_font_size\": 20,\n",
    "        \"label_font_size\": 16,\n",
    "        \"italic\": False,                \n",
    "        \"bold\": True \n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "\n",
    "def pca_whiten(X, n_components=32, seed=0):\n",
    "    \"\"\"PCA -> whiten -> float32 features\"\"\"\n",
    "    pca = PCA(n_components=n_components, whiten=True, random_state=seed)\n",
    "    return pca.fit_transform(np.asarray(X, dtype=np.float32))\n",
    "\n",
    "def l2_normalize(X, eps=1e-12):\n",
    "    n = np.linalg.norm(X, axis=1, keepdims=True)\n",
    "    return X / (n + eps)\n",
    "\n",
    "def idw_upsample(src_pos, src_feat, dst_pos, k=1, eps=1e-12, power=2.0):\n",
    "    \"\"\"Inverse distance weighted upsampling.\"\"\"\n",
    "    nbrs = NearestNeighbors(n_neighbors=k).fit(src_pos)\n",
    "    idx = nbrs.kneighbors(dst_pos, return_distance=False)\n",
    "    d2  = np.sum((dst_pos[:, None, :] - src_pos[idx])**2, axis=-1) + eps\n",
    "    w   = 1.0 / (d2 ** (power/2.0))\n",
    "    w   = w / (w.sum(axis=1, keepdims=True) + eps)\n",
    "    feat_k = src_feat[idx]       # (Nd, k, d)\n",
    "    return np.einsum('nk,nkd->nd', w, feat_k)\n",
    "\n",
    "def kmeans_labels(X, n_clusters=8, seed=0):\n",
    "    km = KMeans(n_clusters=n_clusters, n_init=10, random_state=seed)\n",
    "    return km.fit_predict(X).astype(np.int32)\n",
    "\n",
    "h_enc = model.Enc.register_forward_hook(hook_encoder('Enc'))\n",
    "h_hga = model.Enc.down_convs_hier[-1].register_forward_hook(hook_tensor('HGA_deep'))\n",
    "h_lga = model.Enc.down_convs_local[-1].register_forward_hook(hook_tensor('LGA_last'))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    _ = model(Batch.from_data_list([h3d_set[idx]]).to(cfg['device']))\n",
    "\n",
    "pos_list   = activations['Enc/pos_down']\n",
    "pos_orig   = pos_list[0]                   # (N0, 3)\n",
    "pos_deep   = pos_list[-1]                  # (Nd, 3)\n",
    "H_hga_pool = activations['HGA_deep']       # (Nd, d_h)\n",
    "H_lga_pool = activations['LGA_last']       # (Nd, d_l)\n",
    "\n",
    "# PCA-whiten\n",
    "Z_hga = pca_whiten(H_hga_pool, n_components=32, seed=0)\n",
    "Z_lga = pca_whiten(H_lga_pool, n_components=32, seed=0)\n",
    "\n",
    "# L2 normalization\n",
    "Z_hga = l2_normalize(Z_hga)\n",
    "Z_lga = l2_normalize(Z_lga)\n",
    "\n",
    "# K-means clustering\n",
    "n_clusters = 8\n",
    "labels_hga_pool = kmeans_labels(Z_hga, n_clusters=n_clusters, seed=0)\n",
    "labels_lga_pool = kmeans_labels(Z_lga, n_clusters=n_clusters, seed=0)\n",
    "\n",
    "# 4) Upsample back to original resolution\n",
    "labels_hga_full = idw_upsample(pos_deep, labels_hga_pool[:, None], pos_orig, k=1).ravel().astype(np.int32)\n",
    "labels_lga_full = idw_upsample(pos_deep, labels_lga_pool[:, None], pos_orig, k=1).ravel().astype(np.int32)\n",
    "\n",
    "mesh.cell_data['HGA_clusters'] = labels_hga_full\n",
    "mesh.cell_data['LGA_clusters'] = labels_lga_full\n",
    "\n",
    "annotations_hga = {int(i): f\"HGA-{i}\" for i in range(n_clusters)}\n",
    "annotations_lga = {int(i): f\"LGA-{i}\" for i in range(n_clusters)}\n",
    "\n",
    "# plot\n",
    "mesh.cell_data.active_scalars_name = 'HGA_clusters'\n",
    "mesh.plot(\n",
    "    scalars='HGA_clusters',\n",
    "    cpos='iso',\n",
    "    window_size=(900, 900),\n",
    "    zoom=1.1,\n",
    "    cmap='Set3',\n",
    "    categories=True,\n",
    "    annotations=annotations_hga,\n",
    "    show_axes=False,\n",
    "    show_scalar_bar=True,\n",
    "    scalar_bar_args={\"title\": \"\", \"position_x\": 0.2, \"position_y\": 0.0,\n",
    "                     \"title_font_size\": 20, \"label_font_size\": 16, \"italic\": False, \"bold\": True}\n",
    ")\n",
    "\n",
    "mesh.cell_data.active_scalars_name = 'LGA_clusters'\n",
    "mesh.plot(\n",
    "    scalars='LGA_clusters',\n",
    "    cpos='iso',\n",
    "    window_size=(900, 900),\n",
    "    zoom=1.1,\n",
    "    cmap='Set3',\n",
    "    categories=True,\n",
    "    annotations=annotations_lga,\n",
    "    show_axes=False,\n",
    "    show_scalar_bar=True,\n",
    "    scalar_bar_args={\"title\": \"\", \"position_x\": 0.2, \"position_y\": 0.0,\n",
    "                     \"title_font_size\": 20, \"label_font_size\": 16, \"italic\": False, \"bold\": True}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gen Aggr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"darkgrid\", \n",
    "              palette=\"pastel\")\n",
    "sns.set_theme(rc={\"figure.dpi\": 250, \n",
    "                  'savefig.dpi': 250,\n",
    "                  \"axes.spines.right\": False, \n",
    "                  \"axes.spines.top\": False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_all(arrs):\n",
    "    return [np.asarray(a).reshape(-1) for a in arrs if a is not None]\n",
    "\n",
    "def stack_long(module_name, mode, arrays):\n",
    "    rows = []\n",
    "    for li, a in enumerate(arrays, 1):\n",
    "        if a is None: \n",
    "            continue\n",
    "        flat = np.asarray(a).reshape(-1)\n",
    "        rows += [{\"Module\": module_name, \"Mode\": mode, \"Layer\": li, \"t\": v} for v in flat]\n",
    "    return rows\n",
    "\n",
    "def plot_compact_split_violin(HGA_t_avg, LGA_t_avg, HGA_t_max, LGA_t_max):\n",
    "    df = pd.DataFrame(\n",
    "        stack_long(\"HGA+\", \"t=0.0\", flatten_all(HGA_t_avg)) +\n",
    "        stack_long(\"LGA+\", \"t=0.0\", flatten_all(LGA_t_avg)) +\n",
    "        stack_long(\"HGA+\", \"t=1.0\", flatten_all(HGA_t_max)) +\n",
    "        stack_long(\"LGA+\", \"t=1.0\", flatten_all(LGA_t_max))\n",
    "    )\n",
    "\n",
    "    layers = sorted(df[\"Layer\"].unique())\n",
    "    x_order = []\n",
    "    for l in layers:\n",
    "        x_order += [f\"L{l}\\n(t=0.0)\", f\"L{l}\\n(t=1.0)\"]\n",
    "    df[\"LayerMode\"] = df.apply(lambda r: f\"L{r['Layer']}\\n({r['Mode']})\", axis=1)\n",
    "    df[\"LayerMode\"] = pd.Categorical(df[\"LayerMode\"], categories=x_order, ordered=True)\n",
    "\n",
    "    sns.set_context(\"paper\") \n",
    "    plt.figure(figsize=(9, 4))\n",
    "\n",
    "    ax = sns.violinplot(\n",
    "        data=df, x=\"LayerMode\", y=\"t\",\n",
    "        hue=\"Module\", split=True, inner=\"quartile\", cut=0, linewidth=0.8\n",
    "    )\n",
    "\n",
    "    ax.set_xlabel(\"\", fontsize=10, weight=\"bold\")\n",
    "    ax.set_ylabel(\"t\", fontsize=10, weight=\"bold\")\n",
    "    ax.set_title(\"Learnable t by layer & module â€” HGA+ vs. LGA+\", fontsize=11, weight=\"bold\")\n",
    "\n",
    "    # --- add vertical dotted lines between L1, L2, L3 groups ---\n",
    "    num_per_layer = 2  # each layer has 2 entries (t=0.0, t=1.0)\n",
    "    for i in range(1, len(layers)):  # after L1, after L2\n",
    "        ax.axvline(i * num_per_layer - 0.5, color=\"gray\", linestyle=\"--\", linewidth=0.8)\n",
    "\n",
    "    # legend styling\n",
    "    leg = ax.legend(title=\"Module\", fontsize=9, title_fontsize=9, frameon=False, loc=\"upper left\")\n",
    "    for t in leg.texts: \n",
    "        t.set_fontsize(9)\n",
    "\n",
    "    plt.tight_layout(pad=0.4)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "HGA_t_avg = []\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "  if name in ['Enc.down_convs_hier.0.gen_aggr_avg.t',\n",
    "              'Enc.down_convs_hier.1.gen_aggr_avg.t',\n",
    "              'Enc.down_convs_hier.2.gen_aggr_avg.t']:\n",
    "    HGA_t_avg.append(param.detach().cpu().numpy())\n",
    "\n",
    "LGA_t_avg = []\n",
    "for name, param in model.named_parameters():\n",
    "  if name in ['Enc.down_convs_local.0.gen_aggr_avg.t',\n",
    "              'Enc.down_convs_local.1.gen_aggr_avg.t',\n",
    "              'Enc.down_convs_local.2.gen_aggr_avg.t']:\n",
    "    LGA_t_avg.append(param.detach().cpu().numpy())\n",
    "\n",
    "HGA_t_max = []\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "  if name in ['Enc.down_convs_hier.0.gen_aggr_max.t',\n",
    "              'Enc.down_convs_hier.1.gen_aggr_max.t',\n",
    "              'Enc.down_convs_hier.2.gen_aggr_max.t']:\n",
    "    HGA_t_max.append(param.detach().cpu().numpy())\n",
    "\n",
    "LGA_t_max = []\n",
    "for name, param in model.named_parameters():\n",
    "  if name in ['Enc.down_convs_local.0.gen_aggr_max.t',\n",
    "              'Enc.down_convs_local.1.gen_aggr_max.t',\n",
    "              'Enc.down_convs_local.2.gen_aggr_max.t']:\n",
    "    LGA_t_max.append(param.detach().cpu().numpy())\n",
    "    \n",
    "    \n",
    "plot_compact_split_violin(HGA_t_avg, LGA_t_avg, HGA_t_max, LGA_t_max)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
